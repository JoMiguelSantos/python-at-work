{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Update Salesforce data from Google Sheets.\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "from salesforce_bulk import SalesforceBulk, CsvDictsAdapter\n",
    "from salesforce_bulk.util import IteratorBytesIO\n",
    "from utils.setup import log, vault_credentials\n",
    "from utils import gs\n",
    "from datetime import datetime\n",
    "import httplib2\n",
    "import os\n",
    "from apiclient import discovery\n",
    "from oauth2client import client, tools\n",
    "from oauth2client.file import Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credentials():\n",
    "    \"\"\"Gets valid user credentials from storage.\n",
    "    If nothing has been stored, or if the stored credentials are invalid,\n",
    "    the OAuth2 flow is completed to obtain the new credentials.\n",
    "    Returns:\n",
    "        Credentials, the obtained credential.\n",
    "    \"\"\"\n",
    "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "    CLIENT_SECRET_FILE = 'client_secret.json'\n",
    "\n",
    "    home_dir = os.path.expanduser('~')\n",
    "    credential_dir = os.path.join(home_dir, '.credentials')\n",
    "    if not os.path.exists(credential_dir):\n",
    "        os.makedirs(credential_dir)\n",
    "    credential_path = os.path.join(credential_dir,\n",
    "                                   'sheets.googleapis.com-python-quickstart.json')\n",
    "\n",
    "    store = Storage(credential_path)\n",
    "    credentials = store.get()\n",
    "    if not credentials or credentials.invalid:\n",
    "        flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES)\n",
    "    return credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGS():\n",
    "    credentials = get_credentials()\n",
    "    http = credentials.authorize(httplib2.Http())\n",
    "    discoveryUrl = ('https://sheets.googleapis.com/$discovery/rest?'\n",
    "                    'version=v4')\n",
    "    service = discovery.build('sheets', 'v4', http=http,\n",
    "                              discoveryServiceUrl=discoveryUrl)\n",
    "    return service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_contents(folder_id):\n",
    "    credentials = get_credentials()\n",
    "    http = credentials.authorize(httplib2.Http())\n",
    "    service = discovery.build('drive', 'v3', http=http)\n",
    "\n",
    "    results = service.files().list(\n",
    "        q=(f\"'{folder_id}' in parents and trashed = false\"),\n",
    "#         corpora=\"domain\",\n",
    "        fields=\"files(id, name)\",\n",
    "        supportsTeamDrives=True,\n",
    "        includeTeamDriveItems=True).execute()\n",
    "    items = results.get('files', [])\n",
    "    if not items:\n",
    "        print('No files found.')\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archived_shiftplan_sheet(date):\n",
    "    \n",
    "    #this is the folder where the archived plans are\n",
    "    for folder in get_folder_contents('gsheet_id'): \n",
    "        \n",
    "        #grabs the folder of the corresponding year of the date\n",
    "        if folder['name'] == date.strftime('%Y'):\n",
    "\n",
    "            #gets the shiftplans sheets from that year\n",
    "            shiftplan_folders = get_folder_contents(folder['id'])\n",
    "\n",
    "            for files in shiftplan_folders:\n",
    "\n",
    "                #gets metadata from all files in this folder\n",
    "                sheets_data = buildGS().spreadsheets().get(spreadsheetId=files['id']).execute()\n",
    "\n",
    "                #checks inside each sheet for the tab that contains the specific date\n",
    "                for tab in sheets_data['sheets']:\n",
    "\n",
    "                    if tab['properties']['title'] == date.strftime('%d.%m.%Y'):\n",
    "\n",
    "                        #saves the id from the sheet containing the searched date and returns it\n",
    "                        sheet_id = sheets_data['spreadsheetId']\n",
    "                        return sheet_id\n",
    "    return 'sheet not found'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gs_to_df(sheet_id, tab_name):\n",
    "    gs = buildGS().spreadsheets().values().get(spreadsheetId=sheet_id, range=tab_name.strftime('%d.%m.%Y')).execute()\n",
    "    gs = pd.DataFrame(data=gs['values']) #load google sheet into df\n",
    "    gs = gs.iloc[:,:68] #exclude empty columns\n",
    "    gs.set_index(gs.iloc[:,1], inplace=True) #set specialist names as index\n",
    "    gs = gs.iloc[:gs.index.get_loc('T'),:]\n",
    "    delete_list = [1,2] #include by default row 1 and 2 to delete which are superfluous\n",
    "    gs.fillna(value=\"\", inplace=True)\n",
    "    for i,index in enumerate(gs.index):\n",
    "        if len(index) < 1 | len(index)==None:\n",
    "            delete_list.append(i) #delete empty rows\n",
    "        elif not (\n",
    "            \" \" in index\n",
    "            and \"exclude\" not in index\n",
    "            and \"this\" not in index\n",
    "            and \"and\" not in index\n",
    "            and \"that\" not in index\n",
    "        ):\n",
    "            delete_list.append(i) #delete all rows that are not names of specialists\n",
    "\n",
    "    delete_list.remove(0) #except first row which has the hours\n",
    "    gs.drop(gs.index[delete_list], inplace=True)\n",
    "    gs.rename({i: v  for i,v in enumerate(gs.iloc[0,:])}, axis=1, inplace=True) #rename columns with matching times\n",
    "    gs = gs.T #transpose df\n",
    "    gs.drop(gs.index[0], inplace=True) #drop language row, to be possibly included in future iterations\n",
    "    gs.iloc[:,0] = datetime.strftime(datetime.strptime(gs.iloc[:,0][0], '%d/%m/%Y'),'%m/%d/%Y')\n",
    "    gs.rename({f'{gs.index[0]}': gs.index[0].replace('/','.')}, inplace=True)\n",
    "    gs.reset_index(inplace=True)\n",
    "    gs.rename(columns={'index': gs.iloc[0,0], f'{gs.columns[1]}': str(gs.iloc[0,1])}, inplace=True)\n",
    "    gs.iloc[1:,0] = [datetime.strftime(datetime.strptime(i,'%H:%M'),'%H:%M') for i in gs.iloc[1:,0]]\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_pre_SF(df):\n",
    "    # exclude columns that are not specialist names\n",
    "    for col in df.columns[2:]:\n",
    "        check_val = df.loc[0, col]\n",
    "        if not (\n",
    "            \" \" in check_val\n",
    "            and \"some value\" not in check_val\n",
    "            and \"another value\" not in check_val\n",
    "            and \"yav\" not in check_val\n",
    "            and \"value\" not in check_val\n",
    "        ):\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # use first column as column names\n",
    "    df.columns = df.iloc[0]\n",
    "    # delete first column\n",
    "    df = df.reindex(df.index.drop(0))\n",
    "\n",
    "    # rename first column\n",
    "    first_two_cols = [\"time\", \"date\"]\n",
    "    df.columns.values[[0, 1]] = first_two_cols\n",
    "\n",
    "    df = df.set_index(first_two_cols)\n",
    "    # turn dataframe into series where previous columns are index, drop nas\n",
    "    df = df.stack()\n",
    "\n",
    "    # rename indexes\n",
    "    df.index.names = first_two_cols + [\"name\"]\n",
    "\n",
    "    # turn indexes back to columns\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # create timestamp column and convert to utc\n",
    "    df.loc[:, \"timestamp\"] = pd.to_datetime(\n",
    "        df[\"date\"] + df[\"time\"], format=\"%m/%d/%Y%H:%M\"\n",
    "    )\n",
    "    df[\"timestamp\"] = (\n",
    "        df[\"timestamp\"].dt.tz_localize(\"Europe/Berlin\").dt.tz_convert(\"UTC\")\n",
    "    )\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "\n",
    "    # rename columns to desired final name\n",
    "    df = df.rename(\n",
    "        index=str,\n",
    "        columns={\n",
    "            0: \"column1__c\",\n",
    "            \"timestamp\": \"column2__c\",\n",
    "            \"name\": \"column3__c\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    #drop rows with no activity (null or empty)\n",
    "    df = df[df['column1__c'].apply(len) > 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sf_bulk_job(bulk, object, query=False, insert=False, delete=False):\n",
    "    \"\"\"Wrapper for SF bulk operations.\"\"\"\n",
    "    # Test that one and only one type of action was specified\n",
    "    if sum(map(bool, [query, insert, delete])) != 1:\n",
    "        raise ValueError(\n",
    "            \"Please specify exactly one option - either query, insert or delete.\"\n",
    "        )\n",
    "\n",
    "    # initiate correct job\n",
    "    if query:\n",
    "        job = bulk.create_query_job(object, contentType=\"JSON\")\n",
    "        batch = bulk.query(job, query)\n",
    "    else:\n",
    "        if insert:\n",
    "            job = bulk.create_insert_job(object, contentType=\"CSV\")\n",
    "            batch_list = insert\n",
    "        elif delete:\n",
    "            job = bulk.create_delete_job(object, contentType=\"CSV\")\n",
    "            batch_list = delete\n",
    "\n",
    "        csv_iter = CsvDictsAdapter(iter(batch_list))\n",
    "        batch = bulk.post_batch(job, csv_iter)\n",
    "\n",
    "    bulk.wait_for_batch(job, batch)\n",
    "    bulk.close_job(job)\n",
    "\n",
    "    if query:\n",
    "        results = []\n",
    "        for result in bulk.get_all_results_for_query_batch(batch):\n",
    "            for row in json.load(IteratorBytesIO(result)):\n",
    "                results.append({\"Id\": row[\"Id\"]})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_SF(df):\n",
    "    query = f\"\"\"SELECT something\n",
    "            FROM Table\n",
    "            where StartTime__c > \n",
    "            {datetime.strftime(datetime.strptime(job['time'],'%d.%m.%Y'),\"%Y-%m-%dT00:00:%S.000Z\")}\n",
    "            and StartTime__c < \n",
    "            {datetime.strftime(datetime.strptime(job['time'],'%d.%m.%Y'),\"%Y-%m-%dT23:59:%S.000Z\")}\n",
    "            and isDeleted = False\"\"\"\n",
    "    to_delete = sf_bulk_job(bulk, \"Table\", query=query)\n",
    "\n",
    "    if len(to_delete) > 0:\n",
    "        # delete old data from sf\n",
    "        sf_bulk_job(bulk, \"Table\", delete=to_delete)\n",
    "\n",
    "    if len(df) > 0:  # check that df isn't empty (eg Sundays)\n",
    "        # create primary key from name and time\n",
    "        df.loc[:, \"Name\"] = df[\"date\"] + \" \" + df[\"time\"] + \" \" + df[\"UserName__c\"]\n",
    "\n",
    "        # drop unused columns\n",
    "        df.drop([\"time\", \"date\"], axis=1, inplace=True)\n",
    "\n",
    "        # convert to dict for sf api\n",
    "        inserts = df.to_dict(orient=\"records\")\n",
    "\n",
    "        # insert new data on sf\n",
    "        sf_bulk_job(bulk, \"Table\", insert=inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_to_SF(date_range):\n",
    "    for date in date_range:\n",
    "        sheet_id = get_archived_shiftplan_sheet(date)\n",
    "        \n",
    "        #if the sheet is not in the archived then search in the current shiftplan sheet\n",
    "        if sheet_id == 'sheet not found':\n",
    "            sheet_id = 'gsheet_id'\n",
    "            \n",
    "        df = get_gs_to_df(sheet_id, date)\n",
    "        df = df_to_pre_SF(df)\n",
    "        df_list.append(df)\n",
    "        push_to_SF(df)\n",
    "        df.to_csv(f'df_{date.strftime(\"%d.%m.%Y\")}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "if __name__ == \"__main__\":\n",
    "    gs_to_SF(pd.date_range('02-04-2019','02-05-2019'))\n",
    "    print(df_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
