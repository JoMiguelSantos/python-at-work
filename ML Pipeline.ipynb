{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_engine.missing_data_imputers as mdi\n",
    "from feature_engine import categorical_encoders as ce\n",
    "from feature_engine import discretisers as dsc\n",
    "from feature_engine import outlier_removers as outr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "import psycopg2\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(query):\n",
    "    print('Starting data collection phase...')\n",
    "    # Snowflake\n",
    "    url = URL(\n",
    "        account = 'host',\n",
    "        user = 'username',\n",
    "        password = 'password',\n",
    "        database = 'database_name',\n",
    "        schema = 'schema_name',\n",
    "        warehouse = 'warehouse_name',\n",
    "        role='role_name',\n",
    "    #     authenticator='https://xxxxx.okta.com',\n",
    "    )\n",
    "    engine = create_engine(url)\n",
    "    conn = engine.connect()\n",
    "    #PG\n",
    "    conn = psycopg2.connect(host=\"host\",database=\"prod\", user=\"username\", password=\"password\")\n",
    "    query = '''initial dataset query from DB'''\n",
    "    base_data = pd.read_sql_query(query,conn)\n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_features(df, date, drop_initial_feature=True):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    df['date'] = df[date].dt.strftime('%Y-%m-%d')\n",
    "    df['hour'] = df[date].dt.strftime('%H')\n",
    "    df['dayofweek'] = df[date].dt.strftime('%a')\n",
    "    df['quarter'] = (df[date].dt.quarter).astype(str)\n",
    "    df['month'] = df[date].dt.strftime('%b')\n",
    "    df['year'] = df[date].dt.strftime('%Y')\n",
    "    df['dayofyear'] = df[date].dt.strftime('%-j')\n",
    "    df['dayofmonth'] = df[date].dt.strftime('%-d')\n",
    "    df['weekofyear'] = df[date].dt.strftime('%W')\n",
    "    X = df[['hour','dayofweek','month','year'\n",
    "           'dayofyear','dayofmonth','weekofyear']]\n",
    "    if drop_initial_feature:\n",
    "        df.drop(date,inplace=True, axis=1)\n",
    "    return X.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(data,feature1,feature2):\n",
    "    data[f\"{feature1 + feature2}\"] = data[feature1] + data[feature2]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(missing_values_imputer_list,base_data):\n",
    "    # set up the imputer to transform nulls into a Missing category\n",
    "    imputer = mdi.CategoricalVariableImputer(variables=missing_values_imputer_list)\n",
    "    # fit the imputer\n",
    "    imputer.fit(base_data)\n",
    "    # transform the data\n",
    "    base_data = imputer.transform(base_data)\n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_label_encode(encoder_list,base_data):\n",
    "    # set up the encoder to group rarer labels\n",
    "    encoder = ce.RareLabelCategoricalEncoder(tol=0.01, n_categories=10, variables=encoder_list)\n",
    "    # fit the encoder\n",
    "    encoder.fit(base_data)\n",
    "    # transform the data\n",
    "    base_data = encoder.transform(base_data)\n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(one_hot_list,base_data):\n",
    "    # set up the encoder to OneHotEncode\n",
    "    encoder = ce.OneHotCategoricalEncoder(drop_last=False)\n",
    "    # fit the encoder\n",
    "    encoder.fit(base_data)\n",
    "    # transform the data\n",
    "    base_data = encoder.transform(base_data)\n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(base_data,feature_dimensions=None,feature_combinations=[],features_drop=[],drop_na=True,fill_na=False,\n",
    "            missing_values_imputer_list=[],rare_label_encoder_list=[],one_hot_list=[], scaler=None, date=[]):\n",
    "    print('Starting preprocessing phase...')\n",
    "    if feature_dimensions:\n",
    "        base_data = base_data[feature_dimensions]\n",
    "    else:\n",
    "        feature_dimensions = base_data.columns.values.tolist()\n",
    "    if date in feature_dimensions:\n",
    "        print('Creating date features...')\n",
    "        date_features = create_date_features(base_data, date)\n",
    "    else:\n",
    "        date_features = []\n",
    "    if feature_combinations:\n",
    "        print('Creating feature combinations...')\n",
    "        combine_features(base_data,feature_combinations[0], feature_combinations[1])\n",
    "    print('Imputing missing observations...')\n",
    "    missing_values_imputer_list=base_data.select_dtypes(include='object').columns.tolist()\n",
    "    base_data = impute_missing_values(missing_values_imputer_list,base_data)\n",
    "    if fill_na:\n",
    "        print('Filling NAs...')\n",
    "        base_data[fill_na].fillna(0,inplace=True)\n",
    "    if drop_na:\n",
    "        print('Dropping NAs...')\n",
    "        base_data.dropna(inplace=True)\n",
    "    print('Enconding rare labels...')\n",
    "    rare_label_encoder_list=base_data.select_dtypes(include='object').columns.tolist()\n",
    "    base_data = rare_label_encode(rare_label_encoder_list,base_data)\n",
    "    print('Dropping unnecessary features...')\n",
    "    if features_drop+ (date if date == [] else [date])+feature_combinations != []:\n",
    "        base_data.drop(features_drop+(date if date == [] else [date])+feature_combinations, axis=1, inplace=True)\n",
    "    print('OneHotEncoding features...')\n",
    "    one_hot_list=base_data.select_dtypes(include='object').columns.tolist()\n",
    "    base_data = one_hot_encode(one_hot_list+feature_combinations+date_features,\n",
    "                               base_data) if one_hot_list+feature_combinations+date_features != [] else base_data\n",
    "    if scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(base_data, method, target, k=None, test_size=0.3):\n",
    "    print('Starting feature selection phase...')\n",
    "    # Shuffle data before splitting\n",
    "    base_data = base_data.sample(frac=1).reset_index(drop=True)\n",
    "    if method == 'KBest':\n",
    "        # SelectKBest\n",
    "        X_new = SelectKBest(chi2, k=k).fit_transform(base_data.loc[:, base_data.columns != target], base_data[target])\n",
    "        return  train_test_split(X_new, base_data[target], test_size=test_size)\n",
    "    if method == 'SelectFromRF':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(base_data.loc[:, base_data.columns != target], \n",
    "                                                            base_data[target], test_size=test_size)\n",
    "        RF = RandomForestClassifier(n_estimators = 100)\n",
    "        sel = SelectFromModel(RF)\n",
    "        sel.fit(X_train, y_train)\n",
    "        selected_feat = X_train.columns[(sel.get_support())]\n",
    "        return selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, model_type, model_params=None):\n",
    "    print('Starting model fitting phase...')\n",
    "    if model_type == 'classification':\n",
    "        if 'LR' in locals() or 'LR' in globals():\n",
    "            del LR\n",
    "        LR = LogisticRegression()\n",
    "        LR.fit(X_train, y_train)\n",
    "        if 'RF' in locals():\n",
    "            del RF\n",
    "        RF = RandomForestClassifier()\n",
    "        params = {\"max_depth\": [3, None],\n",
    "                  \"max_features\": ['sqrt', 'log2', len(X_train.columns)],\n",
    "                  \"min_samples_split\": sp_randint(2, 11),\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"],\n",
    "                  \"n_estimators\": [100,200]}\n",
    "        print('Initiating randomized search...')\n",
    "        RRF = RandomizedSearchCV(RF, \n",
    "                                 params, \n",
    "                                 cv=3, \n",
    "                                 n_jobs=-1, \n",
    "                                 scoring='f1',\n",
    "                                 n_iter=50\n",
    "                                )\n",
    "                                 # random search with 33% of the possible combinations if bigger than 100 param combinations possible, else run all combinations \n",
    "#                                  n_iter=np.product([len(v) for v in params.values()])/3 if np.product([len(v) for v in params.values()]) > 100 else np.product([len(v) for v in params.values()]))\n",
    "        RRF.fit(X_train, y_train)\n",
    "        if 'XGB' in locals() or 'XGB' in globals():\n",
    "            del XGB\n",
    "            del RXGB\n",
    "        XGB = xgb.XGBClassifier()\n",
    "        parameters = {\n",
    "             \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "             \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "             \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "             \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "             \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "             }\n",
    "        RXGB = RandomizedSearchCV(\n",
    "                                XGB,\n",
    "                                parameters, \n",
    "                                n_jobs=4,\n",
    "                                scoring=\"neg_log_loss\",\n",
    "                                cv=3)\n",
    "        RXGB.fit(X_train, y_train)\n",
    "        return {'logistic_regression': LR,'randomized Random Forest': RRF.best_estimator_, 'XGBoost': RXGB.best_estimator_}\n",
    "    elif model_type == 'regression':\n",
    "        pass\n",
    "    elif model_type == 'clustering':\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    print('Starting evaluation phase...')\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "    # classification report\n",
    "    cr = classification_report(y_test, model.predict(X_test))\n",
    "    # roc auc\n",
    "    auc = roc_auc_score(y_test, model.predict(X_test))\n",
    "\n",
    "    return [cm,cr,auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(data, models, evaluations, selected_feat):\n",
    "    print('Saving model...')\n",
    "    with open('saved_models.csv','a') as f:\n",
    "        writer = csv.writer(f, delimiter='\\n')\n",
    "        writer.writerow([data.columns.values.tolist(),[v for v in models.values()],evaluations, selected_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection phase...\n",
      "Starting preprocessing phase...\n",
      "Creating feature combinations...\n",
      "Imputing missing observations...\n",
      "Dropping NAs...\n",
      "Enconding rare labels...\n",
      "Dropping unnecessary features...\n",
      "OneHotEncoding features...\n",
      "Starting model fitting phase...\n",
      "Initiating randomized search...\n",
      "Starting evaluation phase...\n",
      "Starting evaluation phase...\n",
      "Starting evaluation phase...\n",
      "[['logistic_regression', [array([[11974,  3110],\n",
      "       [ 7260,  4313]]), '              precision    recall  f1-score   support\\n\\n           0       0.62      0.79      0.70     15084\\n           1       0.58      0.37      0.45     11573\\n\\n    accuracy                           0.61     26657\\n   macro avg       0.60      0.58      0.58     26657\\nweighted avg       0.60      0.61      0.59     26657\\n', 0.5832495260333429, 0.6669167291822956, 0.6033383345836459, 0.25881470367591897, 0.21530382595648911]], ['randomized Random Forest', [array([[10645,  4439],\n",
      "       [ 6595,  4978]]), '              precision    recall  f1-score   support\\n\\n           0       0.62      0.71      0.66     15084\\n           1       0.53      0.43      0.47     11573\\n\\n    accuracy                           0.59     26657\\n   macro avg       0.57      0.57      0.57     26657\\nweighted avg       0.58      0.59      0.58     26657\\n', 0.5679268907276314, 0.5911477869467366, 0.554576144036009, 0.30626406601650413, 0.28244561140285074]], ['XGBoost', [array([[12087,  2997],\n",
      "       [ 7320,  4253]]), '              precision    recall  f1-score   support\\n\\n           0       0.62      0.80      0.70     15084\\n           1       0.59      0.37      0.45     11573\\n\\n    accuracy                           0.61     26657\\n   macro avg       0.60      0.58      0.58     26657\\nweighted avg       0.61      0.61      0.59     26657\\n', 0.5844029762716156, 0.6567891972993248, 0.6114028507126782, 0.24648283624085543, 0.1991747936984246]]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    db_data = get_data(query)\n",
    "    base_data = db_data.copy()\n",
    "    data = preprocess(base_data.drop('received_at',axis=1), \n",
    "                      feature_dimensions=['qualified','gender', 'phone_type', 'phone_country', 'email_provider', 'name_in_email'], \n",
    "                      missing_values_imputer_list=base_data.select_dtypes(include='object').columns.tolist(),\n",
    "                      one_hot_list=base_data.select_dtypes(include='object').columns.tolist(),\n",
    "                      rare_label_encoder_list=base_data.select_dtypes(include='object').columns.tolist(),\n",
    "                      fill_na=base_data.select_dtypes(include=['int','float']).columns.tolist(),\n",
    "                      feature_combinations=['phone_type','phone_country'],\n",
    "                      date='received_at'\n",
    "                     )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.loc[:, data.columns != 'qualified'], data['qualified'], test_size=0.3)\n",
    "    selected_feat = feature_selection(data, 'SelectFromRF','qualified')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[selected_feat.values.tolist()], data['qualified'], test_size=0.3)\n",
    "    models = model(X_train, y_train, 'classification')\n",
    "    evaluations = []\n",
    "    for model_name, model in models.items():\n",
    "        evaluations.append([model_name, evaluate(model, X_test, y_test)])\n",
    "    print(evaluations)\n",
    "    save_model(data, models, evaluations, selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic_regression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'randomized Random Forest': RandomForestClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "                        max_depth=None, max_features=124, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=5,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'XGBoost': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.7, eta=0.05, gamma=0.1,\n",
       "               learning_rate=0.1, max_delta_step=0, max_depth=8,\n",
       "               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "               nthread=None, objective='binary:logistic', random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "               silent=None, subsample=1, verbosity=1)}"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}